% \VignetteIndexEntry{Computational details for ppstat}
% \VignetteKeyword{Computational details}
\documentclass[11pt,twoside]{article}
\usepackage{amssymb}
\usepackage{fancyhdr}

\setlength{\oddsidemargin}{5 mm}
\setlength{\evensidemargin}{5 mm}
\setlength{\textwidth}{150 mm}
\setlength{\textheight}{210 mm}
\setlength{\parskip}{1.3ex}
\setlength{\parindent}{0pt}
\setlength{\headheight}{15pt}

\frenchspacing

\begin{document}

\pagestyle{fancy}

\renewcommand{\sectionmark}[1]{\markright{#1}{}}
\fancyhf{}
\fancyhead[LE,RO]{\bf \thepage}
\fancyhead[LO]{\bf ppstat}
\fancyhead[RE]{\bf }


\thispagestyle{empty} 

\begin{center} 
{\Large \bf Computational details for ppstat } \\ \vskip 3mm

 Niels Richard Hansen\footnote{Postal address: Department of Mathematical Sciences, University
of Copenhagen Universitetsparken 5, 2100 Copenhagen \O, Denmark. \\
Email address: richard@math.ku.dk}, \emph{University of Copenhagen} \\
\end{center}

\section{The likelihood and derivatives}

The core computational problem in the use of point processes for
statistical modeling is the optimization of the minus-log-likelihood
function, which is given as
\begin{eqnarray*}
l(\theta) = \int_{0}^{T}
\lambda_{\theta}(s)\mathrm{d}s - \sum_{j=1}^{n}\log\lambda_{\theta}(t_j)
\end{eqnarray*}
where $0 < t_1 < t_2 < \ldots < t_{n}< T$ are observations and
$\lambda_{\theta}$ is a parameterized family of intensities. Typically
$\theta \in \Theta \subseteq \mathbb{R}^p$. For the Hawkes family of
generalized linear point process models in \verb+ppstat+ we consider
situations where
\begin{eqnarray*}
\lambda_{\theta}(t)=\phi\left( \alpha^TX(t) +\sum_{m=1}^K\sum_{i=1}^{n(m)} h_{\beta^m}^{m}(t-s_i^m)\right)
\end{eqnarray*}
where $\phi:I \to [0,\infty)$, $I \subseteq \mathbb{R}$, is a given
function,
$$\theta = \left(\begin{array}{c}
    \alpha \\ \beta^1 \\ \vdots \\ \beta^K \end{array}\right)$$ 
and $s_1^m < \ldots < s_{n(m)}^m < t$ for $m=1,\ldots,K$ are
observations of point processes (one of these sets of points could be
the $t_i$ observations above). The process $X(t)$ is an auxiliary,
$d(0)$-dimensional observed processes -- observed at least
discretely. The processes
$$\sum_{i=1}^{n(m)} h_{\beta^m}^{m}(t-s_i^m)$$
are linear filters using the (parameterized) filter function $h_{\beta^m}^{m}$, which are given via a
basis expansion
\begin{eqnarray*}
h_{\beta^m}^{m}(t)=(\beta^{m})^TB(t)=\sum_{l=1}^{d(m)}\beta_l^{m}B_l(t),
\end{eqnarray*}
and $\beta^m \in \mathbb{R}^{d(m)}$. Collecting these ingredients --
and interchanging two sums -- the intensity function can be written as
\begin{eqnarray*}
  \lambda_{\theta}(t) & =& \phi\left(  \alpha^TX(t) +\sum_{m=1}^K \sum_{l=1}^{d(m)} \beta_l^{m} \sum_{i=1}^{n(m)}  B_l(t-s_i^m)  \right) = \phi\left( \theta^T Z(t) \right) 
\end{eqnarray*}
with $Z(t)$ a process of dimension $p=d(0) + d(1) + \ldots +
d(K)$. Each of the linear filter components
\begin{eqnarray*}
  \sum_{i=1}^{n(m)}  B_l(t-s_i^m)
\end{eqnarray*}
are computable from the observations and the fixed choice of
basis. The minus-log-likelihood function that we want to minimize
reads
\begin{eqnarray*}
  l(\theta) = \int_{0}^T \phi\left( \theta^T Z(s) \right)  \mathrm{d}s - \sum_{j=1}^{n}\log\phi\left( \theta^T Z(t_j) \right). 
\end{eqnarray*}

The integral is not in general analytically computable. We discretize
time to have a total of $N$ time points and let $Z$ denote the $N
\times p$ matrix of the $Z(t)$-process values at the discretization
points. With $\Delta$ the $N$-dimensional vector of interdistances
from the discretization we arrive at the approximation of the
minus-log-likelihood function that we seek to minimize:
\begin{eqnarray*}
  l(\theta) \simeq  \Delta^T \phi\left( Z \theta \right) - \sum_{j=1}^n \log\phi\left( \theta^T Z(t_j) \right). 
\end{eqnarray*}
We have used the convention that $\phi$ applied to a vector means
coordinate-wise applications of $\phi$. 
Using this expression a precomputation of the $Z$ matrix will allow
for a rapid computation of (the approximation to) $l$. The derivatives 
are likewise approximated as 

\begin{eqnarray*}
  D l(\theta) \simeq [\Delta \circ \phi'\left( Z \theta \right)]^T Z -
  \sum_{j=1}^n \frac{\phi'\left( \theta^T Z(t_j)
    \right)}{\phi\left( \theta^T Z(t_j) \right)}Z(t_j)^T  
\end{eqnarray*}
with $\circ$ the Hadamard (or coordinate-wise) matrix product and
{\small
\begin{eqnarray*}
  D^2l(\theta) \simeq Z^T [\Delta \circ \phi''\left( \theta^T Z
  \right) \circ Z] - \sum_{j=1}^n   \frac{\phi''\left( \theta^T Z(t_j)
    \right) \phi\left( \theta^T Z(t_j) \right) - \phi'\left( \theta^T
      Z(t_j) \right)^2}{\phi\left( \theta^T Z(t_j) \right)^2}  Z(t_j) Z(t_j)^T.
\end{eqnarray*}
}

The formulas above are used for generic computations, but one
particular case can be somewhat simplified. If $\phi = \textnormal{Id}$ is
the identity function then  
$$\Delta^T \phi\left( Z \theta \right) =  (\Delta^T Z) \theta$$
and the $z = \Delta^T Z$ can be precomputed. This is similar
to computing the time integrals of the $B_l$-basis functions
\emph{before} the expansion of the linear filter. 


\section{Algorithms for optimization}

The BFGS and L-BFGS-B algorithms as implemented in R.

The iterative weighted least squares algorithm. 




\end{document}